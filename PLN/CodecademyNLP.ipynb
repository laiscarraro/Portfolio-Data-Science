{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CodecademyNLP.ipynb","provenance":[],"collapsed_sections":["qdqzekA8PpKG","-LQ1igYhTwDg","UV17R20hUKB8","KjPGJhqrUgnd","I9Hpf2prUzzU","GoeoZaWCW9io","EFCs1Na2Y_rF","hrpFpSj4aRox","4HJo0IhgcP19","fQ2J8udgdyPs","4n8Y-cbiugrl","dl9vlpbPwATZ","NVV3M81vxJ0-","OcNvbbPLx2Iw","7BQrZ--3ykQ-","HFGmc-FOzJvT","bdUyV57Lz47e","9I0beFULf5i7","a_B2lcAzglEl","91m4sOWb1SKn","XWyX-UOQ1gBm","kUsexPUx1tWV","OYLmVgHJ2DkS","ZrYSTbwi2R4D"],"authorship_tag":"ABX9TyMv+Bp9Qu96Wf1dZK1jutnV"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6o5WcQ_OLftB"},"source":["#Everyting I learned (and did not memorize) from my 3-month Pro trial in Codecademy\n","\n","Here I'm going to write all my notes and paste all the topics I'm probably not going to remember after a while my subscription has ended, so here we go."]},{"cell_type":"markdown","metadata":{"id":"eShHW8QaSSsQ"},"source":["# 1. Intro to NLP"]},{"cell_type":"markdown","metadata":{"id":"EJbmT6miMPqH"},"source":["## 1.1 Text Preprocessing\n","\n","> \"You never know what you have... until you clean your data.\"\n","*~ Unknown (or possibly made up)*\n","\n","Cleaning and preparation are crucial for many tasks, and NLP is no exception. Text preprocessing is usually the first step you’ll take when faced with an NLP task.\n","\n","Without preprocessing, your computer interprets \"the\", \"The\", and \"<p>The\" as entirely different words. There is a LOT you can do here, depending on the formatting you need. Lucky for you, Regex and NLTK will do most of it for you! Common tasks include:\n","\n","**Noise removal** — stripping text of formatting (e.g., HTML tags).\n","\n","**Tokenization** — breaking text into individual words.\n","\n","**Normalization** — cleaning text data in any other way:\n","\n","Stemming is a blunt axe to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “sing” may become “s” and “sung” would remain “sung.”\n","Lemmatization is a scalpel to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n","Other common tasks include lowercasing, stopwords removal, spelling correction, etc."]},{"cell_type":"code","metadata":{"id":"hSQBhQa6Mo5A"},"source":["# regex for removing punctuation!\n","import re\n","# nltk preprocessing magic\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","# grabbing a part of speech function:\n","from part_of_speech import get_part_of_speech\n","\n","text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n","\n","cleaned = re.sub('\\W+', ' ', text)\n","tokenized = word_tokenize(cleaned)\n","\n","stemmer = PorterStemmer()\n","stemmed = [stemmer.stem(token) for token in tokenized]\n","\n","## -- CHANGE these -- ##\n","lemmatizer = WordNetLemmatizer()\n","lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n","\n","print(\"Stemmed text:\")\n","print(stemmed)\n","print(\"\\nLemmatized text:\")\n","print(lemmatized)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-lJq3MmvMtsv"},"source":["## 1.2 Parsing Text\n","\n","You now have a preprocessed, clean list of words. Now what? It may be helpful to know how the words relate to each other and the underlying syntax (grammar). ***Parsing*** is a stage of NLP concerned with segmenting text based on syntax.\n","\n","You probably do not want to be doing any parsing by hand and NLTK has a few tricks up its sleeve to help you out:\n","\n","***Part-of-speech tagging (POS tagging)*** identifies parts of speech (verbs, nouns, adjectives, etc.). NLTK can do it faster (and maybe more accurately) than your grammar teacher.\n","\n","***Named entity recognition (NER)*** helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. This can be a clue as to the topic of the text and NLTK captures many for you.\n","\n","***Dependency grammar trees*** help you understand the relationship between the words in a sentence. It can be a tedious task for a human, so the Python library spaCy is at your service, even if it isn’t always perfect.\n","\n","In English we leave a lot of ambiguity, so syntax can be tough, even for a computer program. Take a look at the following sentence:\n","\n","> I saw a cow under a tree with binoculars.  \n","\n","Do I have the binoculars? Does the cow have binoculars? Does the tree have binoculars?\n","\n","Regex parsing, using Python’s re library, allows for a bit more nuance. When coupled with POS tagging, you can identify specific phrase chunks. On its own, it can find you addresses, emails, and many other common patterns within large chunks of text."]},{"cell_type":"code","metadata":{"id":"DnJ5BJQaNUSG"},"source":["import spacy\n","from nltk import Tree\n","from squids import squids_text\n","\n","dependency_parser = spacy.load('en')\n","\n","parsed_squids = dependency_parser(squids_text)\n","\n","# Assign my_sentence a new value:\n","my_sentence = \"I saw a spider under a tree with a pretty skirt.\"\n","my_parsed_sentence = dependency_parser(my_sentence)\n","\n","def to_nltk_tree(node):\n","  if node.n_lefts + node.n_rights > 0:\n","    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n","    return Tree(node.orth_, parsed_child_nodes)\n","  else:\n","    return node.orth_\n","\n","for sent in parsed_squids.sents:\n","  to_nltk_tree(sent.root).pretty_print()\n","  \n","for sent in my_parsed_sentence.sents:\n"," to_nltk_tree(sent.root).pretty_print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FGewlltiNXke"},"source":["##1.3 Language Models - Bag-of-Words Approach\n","\n","How can we help a machine make sense of a bunch of word tokens? We can help computers make predictions about language by training a ***language model*** on a corpus (a bunch of example text).\n","\n","**Language models** are probabilistic computer models of language. We build and use these models to figure out the likelihood that a given sound, letter, word, or phrase will be used. Once a model has been trained, it can be tested out on new texts.\n","\n","One of the most common language models is the **unigram** model, a statistical language model commonly known as **bag-of-words**. As its name suggests, bag-of-words does not have much order to its chaos! What it does have is a tally count of each instance for each word. Consider the following text example:\n","\n","The squids jumped out of the suitcases.\n","Provided some initial preprocessing, bag-of-words would result in a mapping like:  \n","\n","```\n","{\"the\": 2, \"squid\": 1, \"jump\": 1, \"out\": 1, \"of\": 1, \"suitcase\": 1}\n","```\n","\n","Now look at this sentence and mapping: “Why are your suitcases full of jumping squids?”\n","\n","```\n","{\"why\": 1, \"be\": 1, \"your\": 1, \"suitcase\": 1, \"full\": 1, \"of\": 1, \"jump\": 1, \"squid\": 1}\n","```\n","\n","You can see how even with different word order and sentence structures, “jump,” “squid,” and “suitcase” are shared topics between the two examples. Bag-of-words can be an excellent way of looking at language when you want to make predictions concerning topic or sentiment of a text. When grammar and word order are irrelevant, this is probably a good model to use."]},{"cell_type":"code","metadata":{"id":"ghduVEtlODxK"},"source":["# importing regex and nltk\n","import re, nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","# importing Counter to get word counts for bag of words\n","from collections import Counter\n","# importing a passage from Through the Looking Glass\n","from looking_glass import looking_glass_text\n","# importing part-of-speech function for lemmatization\n","from part_of_speech import get_part_of_speech\n","\n","# Change text to another string:\n","text = \"So, that's it, I'm here writing this text. This is a very, very nice text, you see. I should say it's the very best text that I've ever wrote in my life. THE very best text. That's it, so, well, bye. That's the end of my text.\"\n","\n","cleaned = re.sub('\\W+', ' ', text).lower()\n","tokenized = word_tokenize(cleaned)\n","\n","stop_words = stopwords.words('english')\n","filtered = [word for word in tokenized if word not in stop_words]\n","\n","normalizer = WordNetLemmatizer()\n","normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n","# Comment out the print statement below\n","# print(normalized)\n","\n","# Define bag_of_looking_glass_words & print:\n","bag_of_looking_glass_words = Counter(normalized)\n","\n","print(bag_of_looking_glass_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YSz8E1DSOJ3Y"},"source":["##1.4 Language Models - N-Grams and NLM\n","\n","For parsing entire phrases or conducting language prediction, you will want to use a model that pays attention to each word’s neighbors. Unlike bag-of-words, the ***n-gram*** model considers a sequence of some number (n) units and calculates the probability of each unit in a body of language given the preceding sequence of length n. Because of this, n-gram probabilities with larger n values can be impressive at language prediction.\n","\n","Take a look at our revised squid example: “The squids jumped out of the suitcases. The squids were furious.”\n","\n","A bigram model (where n is 2) might give us the following count frequencies:\n","\n","```\n","{('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}\n","```\n","\n","There are a couple problems with the n gram model:\n","\n","How can your language model make sense of the sentence “The cat fell asleep in the mailbox” if it’s never seen the word “mailbox” before? During training, your model will probably come across test words that it has never encountered before (this issue also pertains to bag of words). A tactic known as language smoothing can help adjust probabilities for unknown words, but it isn’t always ideal.\n","\n","For a model that more accurately predicts human language patterns, you want n (your sequence length) to be as large as possible. That way, you will have more natural sounding language, right? Well, as the sequence length grows, the number of examples of each sequence within your training corpus shrinks. With too few examples, you won’t have enough data to make many predictions.\n","\n","Enter ***neural language models (NLM)***! Much recent work within NLP has involved developing and training neural networks to approximate the approach our human brains take towards language. This deep learning approach allows computers a much more adaptive tack to processing human language."]},{"cell_type":"code","metadata":{"id":"f72MhVaEO0Vk"},"source":["import nltk, re\n","from nltk.tokenize import word_tokenize\n","# importing ngrams module from nltk\n","from nltk.util import ngrams\n","from collections import Counter\n","from looking_glass import looking_glass_full_text\n","\n","cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n","tokenized = word_tokenize(cleaned)\n","\n","# Change the n value to 2:\n","looking_glass_bigrams = ngrams(tokenized, 2)\n","looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n","\n","# Change the n value to 3:\n","looking_glass_trigrams = ngrams(tokenized, 3)\n","looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n","\n","# Change the n value to a number greater than 3:\n","looking_glass_ngrams = ngrams(tokenized, 8)\n","looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n","\n","print(\"Looking Glass Bigrams:\")\n","print(looking_glass_bigrams_frequency.most_common(10))\n","\n","print(\"\\nLooking Glass Trigrams:\")\n","print(looking_glass_trigrams_frequency.most_common(10))\n","\n","print(\"\\nLooking Glass n-grams:\")\n","print(looking_glass_ngrams_frequency.most_common(10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pnNLsxcDO8OB"},"source":["##1.5 Topic Models\n","\n","We’ve touched on the idea of finding topics within a body of language. But what if the text is long and the topics aren’t obvious?\n","\n","***Topic modeling*** is an area of NLP dedicated to uncovering latent, or hidden, topics within a body of language. For example, one Codecademy curriculum developer used topic modeling to discover patterns within Taylor Swift songs related to love and heartbreak over time.\n","\n","A common technique is to deprioritize the most common words and prioritize less frequently used terms as topics in a process known as **term frequency-inverse document frequency (tf-idf)**. Say what?! This may sound counter-intuitive at first. Why would you want to give more priority to less-used words? Well, when you’re working with a lot of text, it makes a bit of sense if you don’t want your topics filled with words like “the” and “is.” The Python libraries gensim and sklearn have modules to handle tf-idf.\n","\n","Whether you use your plain bag of words (which will give you term frequency) or run it through tf-idf, the next step in your topic modeling journey is often latent Dirichlet allocation (LDA). LDA is a statistical model that takes your documents and determines which words keep popping up together in the same contexts (i.e., documents). We’ll use sklearn to tackle this for us.\n","\n","If you have any interest in visualizing your newly minted topics, word2vec is a great technique to have up your sleeve. word2vec can map out your topic model results spatially as vectors so that similarly used words are closer together. In the case of a language sample consisting of “The squids jumped out of the suitcases. The squids were furious. Why are your suitcases full of jumping squids?”, we might see that “suitcase”, “jump”, and “squid” were words used within similar contexts. This word-to-vector mapping is known as a word embedding."]},{"cell_type":"code","metadata":{"id":"hXcUhi6APLwt"},"source":["import nltk, re\n","from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3\n","from preprocessing import preprocess_text\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","# preparing the text\n","corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n","preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]\n","\n","# Update stop_list:\n","stop_list = ['say', 'know', 'see', 'man', 'well', 'upon', 'one', 'could', 'come', 'may', 'would']\n","# filtering topics for stop words\n","def filter_out_stop_words(corpus):\n","  no_stops_corpus = []\n","  for chapter in corpus:\n","    no_stops_chapter = \" \".join([word for word in chapter.split(\" \") if word not in stop_list])\n","    no_stops_corpus.append(no_stops_chapter)\n","  return no_stops_corpus\n","filtered_for_stops = filter_out_stop_words(preprocessed_corpus)\n","\n","# creating the bag of words model\n","bag_of_words_creator = CountVectorizer()\n","bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n","\n","# creating the tf-idf model\n","tfidf_creator = TfidfVectorizer(min_df = 0.2)\n","tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n","\n","# creating the bag of words LDA model\n","lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n","lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)\n","\n","# creating the tf-idf LDA model\n","lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n","lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)\n","\n","print(\"~~~ Topics found by bag of words LDA ~~~\")\n","for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):\n","  message = \"Topic #{}: \".format(topic_id + 1)\n","  message += \" \".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n","  print(message)\n","\n","print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n","for topic_id, topic in enumerate(lda_tfidf_creator.components_):\n","  message = \"Topic #{}: \".format(topic_id + 1)\n","  message += \" \".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n","  print(message)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1tPXgVLPTAX"},"source":["##1.6 Text Similarity\n","\n","Most of us have a good autocorrect story. Our phone’s messenger quietly swaps one letter for another as we type and suddenly the meaning of our message has changed (to our horror or pleasure). However, addressing text similarity — including spelling correction — is a major challenge within natural language processing.\n","\n","Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering the Levenshtein distance or minimal edit distance between two words. The distance is calculated through the minimum number of insertions, deletions, and substitutions that would need to occur for one word to become another. For example, turning “bees” into “beans” would require one substitution (“a” for “e”) and one insertion (“n”), so the Levenshtein distance would be two.\n","\n","Phonetic similarity is also a major challenge within speech recognition. English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia,” but it’s a far more challenging task for a machine! More advanced autocorrect and spelling correction technology additionally considers key distance on a keyboard and phonetic similarity (how much two words or phrases sound the same).\n","\n","It’s also helpful to find out if texts are the same to guard against plagiarism, which we can identify through lexical similarity (the degree to which texts use the same vocabulary and phrases). Meanwhile, semantic similarity (the degree to which documents contain similar meaning or topics) is useful when you want to find (or recommend) an article or book similar to one you recently finished."]},{"cell_type":"code","metadata":{"id":"NMMCZbd6PbqC"},"source":["import nltk\n","# NLTK has a built-in function\n","# to check Levenshtein distance:\n","from nltk.metrics import edit_distance\n","\n","def print_levenshtein(string1, string2):\n","  print(\"The Levenshtein distance from '{0}' to '{1}' is {2}!\".format(string1, string2, edit_distance(string1, string2)))\n","\n","# Check the distance between\n","# any two words here!\n","print_levenshtein(\"fart\", \"target\")\n","\n","# Assign passing strings here:\n","three_away_from_code = \"coding\"\n","\n","two_away_from_chunk = \"chuml\"\n","\n","print_levenshtein(\"code\", three_away_from_code)\n","print_levenshtein(\"chunk\", two_away_from_chunk)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3U0BlRfPemj"},"source":["##1.7 Language Prediction & Text Generation\n","\n","How does your favorite search engine complete your search queries? How does your phone’s keyboard know what you want to type next? Language prediction is an application of NLP concerned with predicting text given preceding text. Autosuggest, autocomplete, and suggested replies are common forms of language prediction.\n","\n","Your first step to language prediction is picking a language model. Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.\n","\n","If you go the n-gram route, you will most likely rely on Markov chains to predict the statistical likelihood of each following word (or character) based on the training corpus. Markov chains are memory-less and make statistical predictions based entirely on the current n-gram on hand.\n","\n","For example, let’s take a sentence beginning, “I ate so many grilled cheese”. Using a trigram model (where n is 3), a Markov chain would predict the following word as “sandwiches” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data.\n","\n","A more advanced approach, using a neural language model, is the Long Short Term Memory (LSTM) model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them better suited for text prediction than traditional neural networks."]},{"cell_type":"code","metadata":{"id":"kNq0VBl4Plp9"},"source":["import nltk, re, random\n","from nltk.tokenize import word_tokenize\n","from collections import defaultdict, deque\n","from document1 import training_doc1\n","from document2 import training_doc2\n","from document3 import training_doc3\n","\n","class MarkovChain:\n","  def __init__(self):\n","    self.lookup_dict = defaultdict(list)\n","    self._seeded = False\n","    self.__seed_me()\n","\n","  def __seed_me(self, rand_seed=None):\n","    if self._seeded is not True:\n","      try:\n","        if rand_seed is not None:\n","          random.seed(rand_seed)\n","        else:\n","          random.seed()\n","        self._seeded = True\n","      except NotImplementedError:\n","        self._seeded = False\n","    \n","  def add_document(self, str):\n","    preprocessed_list = self._preprocess(str)\n","    pairs = self.__generate_tuple_keys(preprocessed_list)\n","    for pair in pairs:\n","      self.lookup_dict[pair[0]].append(pair[1])\n","  \n","  def _preprocess(self, str):\n","    cleaned = re.sub(r'\\W+', ' ', str).lower()\n","    tokenized = word_tokenize(cleaned)\n","    return tokenized\n","\n","  def __generate_tuple_keys(self, data):\n","    if len(data) < 1:\n","      return\n","\n","    for i in range(len(data) - 1):\n","      yield [ data[i], data[i + 1] ]\n","      \n","  def generate_text(self, max_length=50):\n","    context = deque()\n","    output = []\n","    if len(self.lookup_dict) > 0:\n","      self.__seed_me(rand_seed=len(self.lookup_dict))\n","      chain_head = [list(self.lookup_dict)[0]]\n","      context.extend(chain_head)\n","      \n","      while len(output) < (max_length - 1):\n","        next_choices = self.lookup_dict[context[-1]]\n","        if len(next_choices) > 0:\n","          next_word = random.choice(next_choices)\n","          context.append(next_word)\n","          output.append(context.popleft())\n","        else:\n","          break\n","      output.extend(list(context))\n","    return \" \".join(output)\n","\n","my_markov = MarkovChain()\n","my_markov.add_document(training_doc1)\n","my_markov.add_document(training_doc2)\n","my_markov.add_document(training_doc3)\n","generated_text = my_markov.generate_text()\n","print(generated_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdqzekA8PpKG"},"source":["##1.8 Advanced NLP Topics\n","\n","Believe it or not, you’ve just scratched the surface of natural language processing. There are a slew of advanced topics and applications of NLP, many of which rely on deep learning and neural networks.\n","\n","* ***Naive Bayes classifiers*** are supervised machine learning algorithms that leverage a probabilistic theorem to make predictions and classifications. They are widely used for sentiment analysis (determining whether a given block of language expresses negative or positive feelings) and spam filtering.\n","\n","We’ve made enormous gains in machine translation, but even the most advanced translation software using neural networks and LSTM still has far to go in accurately translating between languages.\n","\n","Some of the most life-altering applications of NLP are focused on improving language accessibility for people with disabilities. Text-to-speech functionality and speech recognition have improved rapidly thanks to neural language models, making digital spaces far more accessible places.\n","\n","NLP can also be used to detect bias in writing and speech. Feel like a political candidate, book, or news source is biased but can’t put your finger on exactly how? Natural language processing can help you identify the language at issue."]},{"cell_type":"code","metadata":{"id":"bdLIwjmiP8SB"},"source":["from reviews import counter, training_counts\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","# Add your review:\n","review = \"This course have been so nice so far. I think all the codecademy courses are utterly amazing. The thing I dislike the least (in other words, the thing I do not dislike most) is that every lesson is practical, and this is just amazing.\"\n","review_counts = counter.transform([review])\n","\n","classifier = MultinomialNB()\n","training_labels = [0] * 1000 + [1] * 1000\n","\n","classifier.fit(training_counts, training_labels)\n","  \n","neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n","pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n","\n","if pos > 50:\n","  print(\"Thank you for your positive review!\")\n","elif neg > 50:\n","  print(\"We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.\")\n","else:\n","  print(\"Naive Bayes cannot determine if this is negative or positive. Thank you or we're sorry?\")\n","\n","  \n","print(\"\\nAccording to our trained Naive Bayes classifier, the probability that your review was negative was {0}% and the probability it was positive was {1}%.\".format(neg, pos))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDN556usS9Wx"},"source":["#2. Regex Review!"]},{"cell_type":"markdown","metadata":{"id":"EPjyncayQToO"},"source":["* ***Regular expressions*** are special sequences of characters that describe a pattern of text that is to be matched\n","\n","* We can use ***literals*** to match the exact characters that we desire\n","\n","* ***Alternation***, using the pipe symbol |, allows us to match the text preceding or following the |\n","\n","* ***Character sets***, denoted by a pair of brackets [], let us match one character from a series of characters\n","\n","* ***Wildcards***, represented by the period or dot ., will match any single character (letter, number, symbol or whitespace)\n","\n","* ***Ranges*** allow us to specify a range of characters in which we can make a match\n","\n","* ***Shorthand character classes*** like \\w, \\d and \\s represent the ranges representing word characters, digit characters, and whitespace characters, respectively\n","\n","* ***Groupings***, denoted with parentheses (), group parts of a regular expression together, and allows us to limit alternation to part of a regex\n","\n","* ***Fixed quantifiers***, represented with curly braces {}, let us indicate the exact quantity or a range of quantity of a character we wish to match\n","\n","* ***Optional quantifiers***, indicated by the question mark ?, allow us to indicate a character in a regex is optional, or can appear either 0 times or 1 time\n","\n","* The ***Kleene star***, denoted with the asterisk *, is a quantifier that matches the preceding character 0 or more times\n","\n","* The ***Kleene plus***, denoted by the plus +, matches the preceding character 1 or more times\n","\n","* The ***anchor*** symbols hat ^ and dollar sign $ are used to match text at the start and end of a string, respectively"]},{"cell_type":"markdown","metadata":{"id":"ZGWnjjdBTIaw"},"source":["#3. Text Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"-LQ1igYhTwDg"},"source":["## 3.0 What we'll see:\n","\n","* ***Text preprocessing*** is all about cleaning and prepping text data so that it’s ready for other NLP tasks.\n","\n","* ***Noise removal*** is a text preprocessing step concerned with removing unnecessary formatting from our text.\n","\n","* ***Tokenization*** is a text preprocessing step devoted to breaking up text into smaller units (usually words or discrete terms).\n","\n","* ***Normalization*** is the name we give most other text preprocessing tasks, including stemming, lemmatization, upper and lowercasing, and stopword removal.\n","\n","* ***Stemming*** is the normalization preprocessing task focused on removing word affixes.\n","\n","* ***Lemmatization*** is the normalization preprocessing task that more carefully brings words down to their root forms."]},{"cell_type":"markdown","metadata":{"id":"UV17R20hUKB8"},"source":["## 3.1 Noise removal\n","\n","We'll see how to remove text noise with the *re* python library."]},{"cell_type":"code","metadata":{"id":"eMFYTZf_UadG"},"source":["import re\n","\n","headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n","\n","tweet = '@fat_meats, veggies are better than you think.'\n","\n","headline_no_tag = re.sub(r'<.?h1>', '', headline_one)\n","\n","tweet_no_at = re.sub(r'@', '', tweet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjPGJhqrUgnd"},"source":["## 3.2 Tokenization\n","\n","We can use python's NLTK to perform the separation of the sentences/words in a text, called ***tokenization***."]},{"cell_type":"code","metadata":{"id":"lbt3C6ItUx2K"},"source":["from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","\n","ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n","\n","tokenized_by_word = word_tokenize(ecg_text)\n","\n","tokenized_by_sentence = sent_tokenize(ecg_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9Hpf2prUzzU"},"source":["## 3.3 Normalization\n","\n","Tokenization and noise removal are staples of almost all text pre-processing pipelines. However, some data may require further processing through text normalization. Text normalization is a catch-all term for various text pre-processing tasks.\n","\n","Steps to normalize a text:\n","\n","* **Upper** or **lower** casing\n","\n","* **Stopword** removal\n","\n","* **POS Tagging** and **Lemmatization**\n","\n","* **Stemming**"]},{"cell_type":"code","metadata":{"id":"8RnWXBLGVhta"},"source":["#Upper or lower casing\n","\n","brands = 'Salvation Army, YMCA, Boys & Girls Club of America'\n","\n","brands_lower = brands.lower()\n","\n","brands_upper = brands.upper()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"teaWZUHlVnAG"},"source":["#Stopword removal\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n","\n","stop_words = set(stopwords.words('english'))\n","\n","tokenized_survey = word_tokenize(survey_text)\n","\n","text_no_stops = [a for a in tokenized_survey if a not in stop_words]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFvE28SQVwK-"},"source":["#Stemming\n","\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import PorterStemmer\n","\n","populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n","\n","stemmer = PorterStemmer()\n","\n","island_tokenized = word_tokenize(populated_island)\n","\n","stemmed = [stemmer.stem(word) for word in island_tokenized]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcsegbbWV5Mg"},"source":["#POS Tagging and Lemmatization\n","\n","import nltk\n","from nltk.corpus import wordnet\n","from collections import Counter\n","\n","def get_part_of_speech(word):\n","  probable_part_of_speech = wordnet.synsets(word)\n","  \n","  pos_counts = Counter()\n","\n","  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n","  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n","  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n","  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n","  \n","  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n","  return most_likely_part_of_speech\n","\n","# Now this is the actual lemmatization but you should do POS Tagging first\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","tokenized_string = word_tokenize(populated_island)\n","\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_string]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5RFwhglYY2p2"},"source":["#A more secure way for POS Tagging:\n","\n","import nltk\n","from nltk import pos_tag\n","from word_tokenized_oz import word_tokenized_oz\n","\n","# save and print the sentence stored at index 100 in word_tokenized_oz here\n","\n","witches_fate = word_tokenized_oz[100]\n","print(witches_fate)\n","\n","# create a list to hold part-of-speech tagged sentences here\n","\n","pos_tagged_oz = []\n","\n","# create a for loop through each word tokenized sentence in word_tokenized_oz here\n","\n","for word in word_tokenized_oz:\n","  pos_tagged_oz.append(pos_tag(word))\n","\n","  # part-of-speech tag each sentence and append to pos_tagged_oz here\n","\n","witches_fate_pos = pos_tagged_oz[100]\n","print(witches_fate_pos)\n","\n","# store and print the 101st part-of-speech tagged sentence here\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNtrar7sWfA7"},"source":["#4. Language Parsing"]},{"cell_type":"markdown","metadata":{"id":"GoeoZaWCW9io"},"source":["##4.1 Compile, match, search and find\n","\n","The first method you will explore is ```.compile()```. This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text. \n","\n","Regular expression objects have a ```.match()``` method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string.\n","\n","If ```.match()``` finds a match that starts at the beginning of the string, it will return a match object. The match object lets you know what piece of text the regular expression matched, and at what index the match begins and ends. If there is no match, ```.match()``` will return None.\n","\n","With the match object stored in result, you can access the matched text by calling ```result.group(0)```. If you use a regex containing capture groups, you can access these groups by calling ```.group()``` with the appropriately numbered capture group as an argument.\n","\n","You can make your regular expression matches even more dynamic with the help of the ```.search()``` method. Unlike ```.match```() which will only find matches at the start of a string, ```.search()``` will look left to right through an entire piece of text and return a match object for the first match to the regular expression given. If no match is found, ```.search()``` will return None.\n","\n","Given a regular expression as its first argument and a string as its second argument, ```.findall()``` will return a list of all non-overlapping matches of the regular expression in the string. "]},{"cell_type":"code","metadata":{"id":"Qo-lqulnYOUv"},"source":["import re\n","\n","# characters are defined\n","character_1 = \"Dorothy\"\n","character_2 = \"Henry\"\n","\n","# compile your regular expression here\n","\n","regular_expression = re.compile('.{7}')\n","\n","# check for a match to character_1 here\n","\n","result_1 = regular_expression.match(character_1)\n","\n","# store and print the matched text here\n","\n","match_1 = result_1.group(0)\n","print(match_1)\n","\n","# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here\n","\n","result_2 = re.match('.{7}', character_2)\n","print(result_2)\n","\n","\n","\n","import re\n","\n","# import L. Frank Baum's The Wonderful Wizard of Oz\n","oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n","\n","# search oz_text for an occurrence of 'wizard' here\n","\n","found_wizard = re.search(\"wizard\", oz_text)\n","print(found_wizard)\n","\n","# find all the occurrences of 'lion' in oz_text here\n","\n","all_lions = re.findall(\"lion\", oz_text)\n","number_lions = len(all_lions)\n","print(number_lions)\n","\n","# store and print the length of all_lions here\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EFCs1Na2Y_rF"},"source":["##4.2 Chunking\n","\n","Given your part-of-speech tagged text, you can now use regular expressions to find patterns in sentence structure that give insight into the meaning of a text. This technique of grouping words by their part-of-speech tag is called chunking.\n","\n","With chunking in nltk, you can define a pattern of parts-of-speech tags using a modified notation of regular expressions. You can then find non-overlapping matches, or chunks of words, in the part-of-speech tagged sentences of a text.\n","\n","One such type of chunking is NP-chunking, or noun phrase chunking. A noun phrase is a phrase that contains a noun and operates, as a unit, as a noun.\n","\n","A popular form of noun phrase begins with a determiner DT, which specifies the noun being referenced, followed by any number of adjectives JJ, which describe the noun, and ends with a noun NN.\n","\n","Another popular type of chunking is VP-chunking, or verb phrase chunking. A verb phrase is a phrase that contains a verb and its complements, objects, or modifiers.\n","\n","Verb phrases can take a variety of structures, and here you will consider two. The first structure begins with a verb VB of any tense, followed by a noun phrase, and ends with an optional adverb RB of any form. The second structure switches the order of the verb and the noun phrase, but also ends with an optional adverb.\n","\n","Another option you have to find chunks in your text is chunk filtering. Chunk filtering lets you define what parts of speech you do not want in a chunk and remove them."]},{"cell_type":"code","metadata":{"id":"AZm0YfpwZRYD"},"source":["#chunking basics\n","\n","from nltk import RegexpParser, Tree\n","from pos_tagged_oz import pos_tagged_oz\n","\n","# define adjective-noun chunk grammar here\n","\n","chunk_grammar = \"AN: {<JJ><NN>}\"\n","\n","# create RegexpParser object here\n","\n","chunk_parser = RegexpParser(chunk_grammar)\n","\n","# chunk the pos-tagged sentence at index 282 in pos_tagged_oz here\n","\n","scaredy_cat = chunk_parser.parse(pos_tagged_oz[282])\n","print(scaredy_cat)\n","\n","# pretty_print the chunked sentence here\n","Tree.fromstring(str(scaredy_cat)).pretty_print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNOy83bfZgER"},"source":["#NP Chunking\n","\n","from nltk import RegexpParser\n","from pos_tagged_oz import pos_tagged_oz\n","from np_chunk_counter import np_chunk_counter\n","\n","# define noun-phrase chunk grammar here\n","\n","chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","\n","# create RegexpParser object here\n","\n","chunk_parser = RegexpParser(chunk_grammar)\n","\n","# create a list to hold noun-phrase chunked sentences\n","np_chunked_oz = list()\n","\n","# create a for loop through each pos-tagged sentence in pos_tagged_oz here\n","for sentence in pos_tagged_oz:\n","  # chunk each sentence and append to np_chunked_oz here\n","  np_chunked_oz.append(chunk_parser.parse(sentence))\n","\n","# store and print the most common np-chunks here\n","\n","most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n","\n","print(most_common_np_chunks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dANbubvHZic4"},"source":["#VP Chunking\n","\n","from nltk import RegexpParser\n","from pos_tagged_oz import pos_tagged_oz\n","from vp_chunk_counter import vp_chunk_counter\n","\n","# define verb phrase chunk grammar here\n","\n","chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n","\n","# create RegexpParser object here\n","\n","chunk_parser = RegexpParser(chunk_grammar)\n","\n","# create a list to hold verb-phrase chunked sentences\n","vp_chunked_oz = list()\n","\n","# create for loop through each pos-tagged sentence in pos_tagged_oz here\n","for p in pos_tagged_oz:\n","  # chunk each sentence and append to vp_chunked_oz here\n","  vp_chunked_oz.append(chunk_parser.parse(p))\n","  \n","# store and print the most common vp-chunks here\n","most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n","print(most_common_vp_chunks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoUJKvyEZ0YL"},"source":["#Chunk filtering\n","\n","from nltk import RegexpParser, Tree\n","from pos_tagged_oz import pos_tagged_oz\n","\n","# define chunk grammar to chunk an entire sentence together\n","grammar = \"Chunk: {<.*>+}\"\n","\n","# create RegexpParser object\n","parser = RegexpParser(grammar)\n","\n","# chunk the pos-tagged sentence at index 230 in pos_tagged_oz\n","chunked_dancers = parser.parse(pos_tagged_oz[230])\n","print(chunked_dancers)\n","\n","# define noun phrase chunk grammar using chunk filtering here\n","\n","chunk_grammar = \"\"\"NP: {<.*>+}\n","}<VB.*|IN>+{\"\"\"\n","\n","# create RegexpParser object here\n","\n","chunk_parser = RegexpParser(chunk_grammar)\n","\n","# chunk and filter the pos-tagged sentence at index 230 in pos_tagged_oz here\n","\n","filtered_dancers = chunk_parser.parse(pos_tagged_oz[230])\n","print(filtered_dancers)\n","\n","# pretty_print the chunked and filtered sentence here\n","Tree.fromstring(str(filtered_dancers)).pretty_print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_Z4XjCCaO7D"},"source":["#5. Language models"]},{"cell_type":"markdown","metadata":{"id":"hrpFpSj4aRox"},"source":["##5.1 Bag-of-Words\n","\n","Bag-of-words (BoW) is a statistical language model based on word count. Bag-of-words does not give a flying fish about word starts or word order though; its sole concern is word count — how many times each word appears in a document.\n","\n","One of the most common ways to implement the BoW model in Python is as a dictionary with each key set to a word and each value set to the number of times that word appears. For statistical models, we call the text that we use to build the model our training data.\n","\n","###**BoW Vectors**\n","\n","A feature vector is a numeric representation of an item’s important features. Each feature has its own column. If the feature exists for the item, you could represent that with a 1. If the feature does not exist for that item, you could represent that with a 0. Turning text into a BoW vector is known as feature extraction or vectorization.\n","\n","But how do we know which vector index corresponds to which word? When building BoW vectors, we generally create a features dictionary of all vocabulary in our training data (usually several documents) mapped to indices."]},{"cell_type":"code","metadata":{"id":"4_bxUE7fbcjq"},"source":["# Creating a features dictionary (you can do this with less code bellow)\n","\n","from preprocessing import preprocess_text\n","# Define create_features_dictionary() below:\n","\n","def create_features_dictionary(documents):\n","  features_dictionary = dict()\n","  merged = \" \".join(documents)\n","  tokens = preprocess_text(merged)\n","  index = 0\n","  for token in tokens:\n","    if(token not in features_dictionary):\n","      features_dictionary[token] = index\n","      index += 1\n","  return features_dictionary, tokens\n","\n","training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n","\n","print(create_features_dictionary(training_documents)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4m1dxJ-lbkHK"},"source":["# Creating a BoW vector (you can do this with less code bellow)\n","\n","from preprocessing import preprocess_text\n","# Define text_to_bow_vector() below:\n","\n","def text_to_bow_vector(some_text, features_dictionary):\n","  bow_vector = [0] * len(features_dictionary)\n","  tokens = preprocess_text(some_text)\n","  for token in tokens:\n","    feature_index = features_dictionary[token]\n","    bow_vector[feature_index] += 1\n","  return bow_vector, tokens\n","\n","features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n","\n","text = \"Another five fish find another faraway fish.\"\n","print(text_to_bow_vector(text, features_dictionary)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjGzWyJ7bnVO"},"source":["# Creating a Naiive Bayes classifier (you can do this with less code bellow)\n","\n","from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n","from sklearn.naive_bayes import MultinomialNB\n","\n","def create_features_dictionary(document_tokens):\n","  features_dictionary = {}\n","  index = 0\n","  for token in document_tokens:\n","    if token not in features_dictionary:\n","      features_dictionary[token] = index\n","      index += 1\n","  return features_dictionary\n","\n","def tokens_to_bow_vector(document_tokens, features_dictionary):\n","  bow_vector = [0] * len(features_dictionary)\n","  for token in document_tokens:\n","    if token in features_dictionary:\n","      feature_index = features_dictionary[token]\n","      bow_vector[feature_index] += 1\n","  return bow_vector\n","\n","# Define bow_sms_dictionary:\n","\n","bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n","\n","# Define training_vectors:\n","\n","training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]\n","\n","# Define test_vectors:\n","\n","test_vectors = [tokens_to_bow_vector(test_doc, bow_sms_dictionary) for test_doc in test_spam_docs]\n","\n","spam_classifier = MultinomialNB()\n","\n","def spam_or_not(label):\n","  return \"spam\" if label else \"not spam\"\n","\n","# Uncomment the code below when you're done:\n","spam_classifier.fit(training_vectors, training_labels)\n","\n","predictions = spam_classifier.score(test_vectors, test_labels)\n","\n","print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100, test_docs[0], spam_or_not(test_labels[0]), test_docs[10], spam_or_not(test_labels[10])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9MFDC-zcDld"},"source":["# And here's how you do it with less code!\n","\n","from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n","from sklearn.naive_bayes import MultinomialNB\n","\n","# Import CountVectorizer from sklearn:\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Define bow_vectorizer:\n","\n","bow_vectorizer = CountVectorizer()\n","\n","# Define training_vectors:\n","\n","training_vectors = bow_vectorizer.fit_transform(training_docs)\n","\n","# Define test_vectors:\n","\n","test_vectors = bow_vectorizer.transform(test_docs)\n","\n","spam_classifier = MultinomialNB()\n","\n","def spam_or_not(label):\n","  return \"spam\" if label else \"not spam\"\n","\n","# Uncomment the code below when you're done:\n","spam_classifier.fit(training_vectors, training_labels)\n","\n","predictions = spam_classifier.score(test_vectors, test_labels)\n","\n","print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100, test_docs[7], spam_or_not(test_labels[7]), test_docs[35], spam_or_not(test_labels[11])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4HJo0IhgcP19"},"source":["## 5.2 TF-IDF\n","\n","Term frequency-inverse document frequency is a numerical statistic used to indicate how important a word is to each document in a collection of documents, or a corpus.\n","\n","When applying tf-idf to a corpus, each word is given a tf-idf score for each document, representing the relevance of that word to the particular document. A higher tf-idf score indicates a term is more important to the corresponding document.\n","\n","The first component of tf-idf is term frequency, or how often a word appears in a document within the corpus.\n","\n","The inverse document frequency component of the tf-idf score penalizes terms that appear more frequently across a corpus. The intuition is that words that appear more frequently in the corpus give less insight into the topic or meaning of an individual document, and should thus be deprioritized.\n","\n","tfidf(t,d) = tf(t,d)*idf(t,corpus)tfidf(t,d)=tf(t,d)∗idf(t,corpus)"]},{"cell_type":"code","metadata":{"id":"32ApIAaOc13H"},"source":["# Calculate Term Frequency\n","\n","import codecademylib3_seaborn\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from preprocessing import preprocess_text\n","\n","poem = '''\n","Success is counted sweetest\n","By those who ne'er succeed.\n","To comprehend a nectar\n","Requires sorest need.\n","\n","Not one of all the purple host\n","Who took the flag to-day\n","Can tell the definition,\n","So clear, of victory,\n","\n","As he, defeated, dying,\n","On whose forbidden ear\n","The distant strains of triumph\n","Break, agonized and clear!'''\n","\n","# define clear_count:\n","clear_count = 2\n","\n","# preprocess text\n","processed_poem = preprocess_text(poem)\n","\n","# initialize and fit CountVectorizer\n","vectorizer = CountVectorizer()\n","term_frequencies = vectorizer.fit_transform([processed_poem])\n","\n","# get vocabulary of terms\n","\n","feature_names = vectorizer.get_feature_names()\n","\n","# create pandas DataFrame with term frequencies\n","try:\n","  df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=['Term Frequency'])\n","  print(df_term_frequencies)\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSm5ElmXc5-n"},"source":["# Calculate the Inverse Document Frequency\n","\n","import codecademylib3_seaborn\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from term_frequency import term_frequencies, feature_names, df_term_frequencies\n","\n","# display term-document matrix of term frequencies\n","\n","print(df_term_frequencies)\n","\n","# initialize and fit TfidfTransformer\n","\n","transformer = TfidfTransformer()\n","transformer.fit(term_frequencies)\n","idf_values = transformer.idf_\n","\n","# create pandas DataFrame with inverse document frequencies\n","try:\n","  df_idf = pd.DataFrame(idf_values, index = feature_names, columns=['Inverse Document Frequency'])\n","  print(df_idf)\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jBq_mrRdP-p"},"source":["# Calculating everything\n","\n","import codecademylib3_seaborn\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from poems import poems\n","from preprocessing import preprocess_text\n","\n","# preprocess documents\n","processed_poems = [preprocess_text(poem) for poem in poems]\n","\n","# initialize and fit TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(norm=None)\n","tfidf_scores = vectorizer.fit_transform(processed_poems)\n","\n","# get vocabulary of terms\n","\n","feature_names = vectorizer.get_feature_names()\n","\n","# get corpus index\n","corpus_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n","\n","# create pandas DataFrame with tf-idf scores\n","try:\n","  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=corpus_index)\n","  print(df_tf_idf)\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzmdOsFtdXYR"},"source":["# BoW to TF-IDF tranformation\n","\n","import codecademylib3_seaborn\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from term_frequency import bow_matrix, feature_names, df_bag_of_words, corpus_index\n","\n","# display term-document matrix of term frequencies (bag-of-words)\n","\n","print(df_bag_of_words)\n","\n","# initialize and fit TfidfTransformer, transform bag-of-words matrix\n","\n","transformer = TfidfTransformer(norm=None)\n","tfidf_scores = transformer.fit_transform(bow_matrix)\n","\n","# create pandas DataFrame with tf-idf scores\n","try:\n","  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index = feature_names, columns=corpus_index)\n","  print(df_tf_idf)\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-gLHAfudkAV"},"source":["#6. Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"fQ2J8udgdyPs"},"source":["##6.1 Word Embeddings\n","\n","Word embeddings are vector representations of a word.\n","\n","They allow us to take all the information that is stored in a word, like its meaning and its part of speech, and convert it into a numeric form that is more understandable to a computer.\n","\n","The key at the heart of word embeddings is distance. We'll cover Manhatthan, Euclidean and Cosine, but Cosine is preferable as the length of the vector does not influence on the number of calculations made.\n","\n","Word2vec is a statistical learning algorithm that develops word embeddings from a corpus of text. With either the continuous bag-of-words or continuous skip-grams representations as training data, word2vec then uses a shallow, 2-layer neural network to come up with the values that place words with a similar context in vectors near each other and words with different contexts in vectors far apart from each other.\n","\n","When we want to train our own word2vec model on a corpus of text, we can use the gensim package!"]},{"cell_type":"code","metadata":{"id":"laH8j3vFd9Zo"},"source":["# Basic word embeddings with spacy\n","\n","import spacy\n","\n","# load word embedding model\n","\n","nlp = spacy.load('en')\n","\n","# define word embedding vectors\n","\n","happy_vec = nlp('happy').vector\n","sad_vec = nlp('sad').vector\n","angry_vec = nlp('angry').vector\n","\n","#print(happy_vec)\n","\n","# find vector length here\n","\n","vector_length = len(happy_vec)\n","print(vector_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IAHvJduebWK"},"source":["# Finding different distances\n","\n","import numpy as np\n","from scipy.spatial.distance import cityblock, euclidean, cosine\n","import spacy\n","\n","# load word embedding model\n","nlp = spacy.load('en')\n","\n","# define word embedding vectors\n","happy_vec = nlp('happy').vector\n","sad_vec = nlp('sad').vector\n","angry_vec = nlp('angry').vector\n","\n","# calculate Manhattan distance\n","\n","man_happy_sad = cityblock(happy_vec, sad_vec)\n","man_sad_angry = cityblock(sad_vec, angry_vec)\n","\n","print(man_happy_sad)\n","print(man_sad_angry)\n","\n","# calculate Euclidean distance\n","\n","euc_happy_sad = euclidean(happy_vec, sad_vec)\n","euc_sad_angry = euclidean(sad_vec, angry_vec)\n","\n","print(euc_happy_sad)\n","print(euc_sad_angry)\n","\n","# calculate cosine distance\n","\n","cos_happy_sad = cosine(happy_vec, sad_vec)\n","cos_sad_angry = cosine(sad_vec, angry_vec)\n","\n","print(cos_happy_sad)\n","print(cos_sad_angry)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nl2KU0Cheed2"},"source":["# Word2Vec\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","sentence = \"It was the best of times, it was the worst of times.\"\n","print(sentence)\n","\n","# preprocessing\n","sentence_lst = [word.lower().strip(\".\") for word in sentence.split()]\n","\n","# set context_length\n","context_length = 4\n","\n","# function to get cbows\n","def get_cbows(sentence_lst, context_length):\n","  cbows = list()\n","  for i, val in enumerate(sentence_lst):\n","    if i < context_length:\n","      pass\n","    elif i < len(sentence_lst) - context_length:\n","      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n","      vectorizer = CountVectorizer()\n","      vectorizer.fit_transform(context)\n","      context_no_order = vectorizer.get_feature_names()\n","      cbows.append((val,context_no_order))\n","  return cbows\n","\n","# define cbows here:\n","\n","cbows = get_cbows(sentence_lst, context_length)\n","\n","# function to get cbows\n","def get_skip_grams(sentence_lst, context_length):\n","  skip_grams = list()\n","  for i, val in enumerate(sentence_lst):\n","    if i < context_length:\n","      pass\n","    elif i < len(sentence_lst) - context_length:\n","      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n","      skip_grams.append((val, context))\n","  return skip_grams\n","\n","# define skip_grams here:\n","\n","skip_grams = get_skip_grams(sentence_lst, context_length)\n","\n","try:\n","  print('\\nContinuous Bag of Words')\n","  for cbow in cbows:\n","    print(cbow)\n","except:\n","  pass\n","try:\n","  print('\\nSkip Grams')\n","  for skip_gram in skip_grams:\n","    print(skip_gram)\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90usKpyhe-Pu"},"source":["# Gensim\n","\n","import gensim\n","from nltk.corpus import stopwords\n","from romeo_juliet import romeo_and_juliet\n","\n","# load stop words\n","stop_words = stopwords.words('english')\n","\n","# preprocess text\n","romeo_and_juliet_processed = [[word for word in romeo_and_juliet.lower().split() if word not in stop_words]]\n","\n","# view inner list of romeo_and_juliet_processed\n","\n","#print(romeo_and_juliet_processed[0][:20])\n","\n","# train word embeddings model\n","\n","model = gensim.models.Word2Vec(romeo_and_juliet_processed, size = 100, window = 5, min_count = 1, workers = 2, sg = 1)\n","\n","# view vocabulary\n","\n","vocabulary = list(model.wv.vocab.items())\n","#print(vocabulary)\n","\n","# similar to romeo\n","\n","similar_to_romeo = model.most_similar(\"romeo\", topn = 20)\n","print(similar_to_romeo)\n","\n","# one is not like the others\n","\n","not_star_crossed_lover = model.doesnt_match([\"romeo\", \"juliet\", \"mercutio\"])\n","print(not_star_crossed_lover)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iYfzW52dfpR3"},"source":["# 8. Deep Learning"]},{"cell_type":"markdown","metadata":{"id":"4n8Y-cbiugrl"},"source":["##8.0. Preprocessing for seq2seq\n","Noise removal depends on your use case — do you care about casing or punctuation? For many tasks they are probably not important enough to justify the additional processing. This might be the time to make changes.\n","\n","We’ll need the following for our Keras implementation:\n","\n","- vocabulary sets for both our input (English) and target (Spanish) data\n","- the total number of unique word tokens we have for each set\n","- the maximum sentence length we’re using for each language\n","\n","We also need to mark the start and end of each document (sentence) in the target samples so that the model recognizes where to begin and end its text generation (no book-long sentences for us!). One way to do this is adding \"<START>\" at the beginning and \"<END>\" at the end of each target document (in our case, this will be our Spanish sentences). For example, \"Estoy feliz.\" becomes \"<START> Estoy feliz. <END>\".\n","\n"]},{"cell_type":"code","metadata":{"id":"3bL20rVfvCWN"},"source":["from tensorflow import keras\n","import re\n","# Importing our translations\n","data_path = \"span-eng.txt\"\n","# Defining lines as a list of each line\n","with open(data_path, 'r', encoding='utf-8') as f:\n","  lines = f.read().split('\\n')\n","\n","# Building empty lists to hold sentences\n","input_docs = []\n","target_docs = []\n","# Building empty vocabulary sets\n","input_tokens = set()\n","target_tokens = set()\n","\n","for line in lines:\n","  # Input and target sentences are separated by tabs\n","  input_doc, target_doc = line.split('\\t')\n","  # Appending each input sentence to input_docs\n","  input_docs.append(input_doc)\n","  # Splitting words from punctuation\n","  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n","  # Redefine target_doc below \n","  # and append it to target_docs:\n","  target_doc = \"<START>\" + target_doc + \"<END>\"\n","  target_docs.append(target_doc)\n","  \n","  # Now we split up each sentence into words\n","  # and add each unique word to our vocabulary set\n","  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n","    print(token)\n","    # Add your code here:\n","    if token not in input_tokens:\n","      input_tokens.add(token)\n","    \n","  for token in target_doc.split():\n","    print(token)\n","    # And here:\n","    if token not in target_tokens:\n","      target_tokens.add(token)\n","\n","input_tokens = sorted(list(input_tokens))\n","target_tokens = sorted(list(target_tokens))\n","\n","# Create num_encoder_tokens and num_decoder_tokens:\n","num_encoder_tokens = len(input_tokens)\n","num_decoder_tokens = 27\n","\n","try:\n","  max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n","  max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n","except ValueError:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgVZglGSvF5L"},"source":["#data:\n","'''\n","We'll see.\tDespués veremos.\n","We'll see.\tYa veremos.\n","We'll try.\tLo intentaremos.\n","We've won!\t¡Hemos ganado!\n","Well done.\tBien hecho.\n","What's up?\t¿Qué hay?\n","Who cares?\t¿A quién le importa?\n","Who drove?\t¿Quién condujo?\n","Who drove?\t¿Quién conducía?\n","Who is he?\t¿Quién es él?\n","Who is it?\t¿Quién es?\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dl9vlpbPwATZ"},"source":["##9.1 Training Setup \n","For each sentence, Keras expects a NumPy matrix containing one-hot vectors for each token. What’s a one-hot vector? In a one-hot vector, every token in our set is represented by a 0 except for the current token which is represented by a 1. For example given the vocabulary [\"the\", \"dog\", \"licked\", \"me\"], a one-hot vector for “dog” would look like [0, 1, 0, 0].\n","\n","In order to vectorize our data and later translate it from vectors, it’s helpful to have a features dictionary (and a reverse features dictionary) to easily translate between all the 1s and 0s and actual words. We’ll build out the following:\n","\n","- a features dictionary for English\n","- a features dictionary for Spanish\n","- a reverse features dictionary for English (where the keys and values are swapped)\n","- a reverse features dictionary for Spanish\n","\n","Once we have all of our features dictionaries set up, it’s time to vectorize the data! We’re going to need vectors to input into our encoder and decoder, as well as a vector of target data we can use to train the decoder.\n","\n","Because each matrix is almost all zeros, we’ll use numpy.zeros() from the NumPy library to build them out.\n","\n","At this point we need to fill out the 1s in each vector. We can loop over each English-Spanish pair in our training sample using the features dictionaries to add a 1 for the token in question. \n","\n","Keras will fit — or train — the seq2seq model using these matrices of one-hot vectors:\n","\n","- the encoder input data\n","- the decoder input data\n","- the decoder target data\n","\n","Hang on a second, why build two matrices of decoder data? Aren’t we just encoding and decoding?\n","\n","The reason has to do with a technique known as teacher forcing that most seq2seq models employ during training. Here’s the idea: we have a Spanish input token from the previous timestep to help train the model for the current timestep’s target token."]},{"cell_type":"code","metadata":{"id":"-DoEEsLzwjq2"},"source":["from tensorflow import keras\n","import numpy as np\n","from preprocessing import input_docs, target_docs, input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n","\n","print('Number of samples:', len(input_docs))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)\n","\n","input_features_dict = dict(\n","    [(token, i) for i, token in enumerate(input_tokens)])\n","\n","# Build out target_features_dict:\n","target_features_dict = dict(\n","    [(token, i) for i, token in enumerate(target_tokens)])\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_features_dict = dict(\n","    (i, token) for token, i in input_features_dict.items())\n","\n","# Build out reverse_target_features_dict:\n","reverse_target_features_dict = dict(\n","    (i, token) for token, i in target_features_dict.items())\n","\n","encoder_input_data = np.zeros(\n","    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","print(\"\\nHere's the first item in the encoder input matrix:\\n\", encoder_input_data[0], \"\\n\\nThe number of columns should match the number of unique input tokens and the number of rows should match the maximum sequence length for input sentences.\")\n","\n","# Build out the decoder_input_data matrix:\n","decoder_input_data = np.zeros(\n","    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","print(\"\\nHere's the first item in the decoder input matrix:\\n\", decoder_input_data[0], \"\\n\\nThe number of columns should match the number of unique input tokens and the number of rows should match the maximum sequence length for input sentences.\")\n","\n","# Build out the decoder_target_data matrix:\n","decoder_target_data = np.zeros(\n","    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","print(\"\\nHere's the first item in the decoder input matrix:\\n\", decoder_input_data[0], \"\\n\\nThe number of columns should match the number of unique input tokens and the number of rows should match the maximum sequence length for input sentences.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgdypwmGxC63"},"source":["from tensorflow import keras\n","import numpy as np\n","import re\n","from preprocessing import input_docs, target_docs, input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n","\n","input_features_dict = dict(\n","    [(token, i) for i, token in enumerate(input_tokens)])\n","target_features_dict = dict(\n","    [(token, i) for i, token in enumerate(target_tokens)])\n","\n","reverse_input_features_dict = dict(\n","    (i, token) for token, i in input_features_dict.items())\n","reverse_target_features_dict = dict(\n","    (i, token) for token, i in target_features_dict.items())\n","\n","encoder_input_data = np.zeros(\n","    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","\n","for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n","\n","  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n","\n","    print(\"Encoder input timestep & token:\", timestep, token)\n","    print(input_features_dict[token])\n","    # Assign 1. for the current line, timestep, & word\n","    # in encoder_input_data:\n","    encoder_input_data[line, timestep, input_features_dict[token]] = 1\n","\n","  for timestep, token in enumerate(target_doc.split()):\n","\n","    # decoder_target_data is ahead of decoder_input_data by one timestep\n","    print(\"Decoder input timestep & token:\", timestep, token)\n","    # Assign 1. for the current line, timestep, & word\n","    # in decoder_input_data:\n","    decoder_input_data[line, timestep, target_features_dict[token]] = 1\n","    \n","    if timestep > 0:\n","      # decoder_target_data is ahead by 1 timestep\n","      # and doesn't include the start token.\n","      decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1\n","      print(\"Decoder target timestep:\", timestep)\n","      # Assign 1. for the current line, timestep, & word\n","      # in decoder_target_data:\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVV3M81vxJ0-"},"source":["##9.2 Encoder Training Setup\n","\n","Deep learning models in Keras are built in layers, where each layer is a step in the model.\n","\n","Our encoder requires two layer types from Keras:\n","\n","- An input layer, which defines a matrix to hold all the one-hot vectors that we’ll feed to the model.\n","- An LSTM layer, with some output dimensionality.\n","\n","Next, we set up the input layer, which requires some number of dimensions that we’re providing. In this case, we know that we’re passing in all the encoder tokens, but we don’t necessarily know our batch size (how many ~chocolate chip cookies~ sentences we’re feeding the model at a time). Fortunately, we can say None because the code is written to handle varying batch sizes, so we don’t need to specify that dimension.\n","\n","For the LSTM layer, we need to select the dimensionality (the size of the LSTM’s hidden states, which helps determine how closely the model molds itself to the training data — something we can play around with) and whether to return the state.\n","\n","Remember, the only thing we want from the encoder is its final states. encoder_outputs isn’t really important for us, so we can just discard it. However, the states, we’ll save in a list. There is a lot to take in here, but there’s no need to memorize any of this — you got this.💪"]},{"cell_type":"code","metadata":{"id":"fWzPfFLexxwj"},"source":["from prep import num_encoder_tokens\n","\n","from tensorflow import keras\n","from keras.layers import Input, LSTM\n","from keras.models import Model\n","\n","# Create the input layer:\n","encoder_inputs = Input(shape = (None, num_encoder_tokens))\n","\n","# Create the LSTM layer:\n","encoder_lstm = LSTM(256, return_state = True)\n","\n","# Retrieve the outputs and states:\n","encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n","\n","# Put the states together in a list:\n","encoder_states = [state_hidden, state_cell]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OcNvbbPLx2Iw"},"source":["##9.3 Decoder Training Setup\n","\n","The decoder looks a lot like the encoder (phew!), with an input layer and an LSTM layer that we use together. However, with our decoder, we pass in the state data from the encoder, along with the decoder inputs. This time, we’ll keep the output instead of the states. \n","\n","We also need to run the output through a final activation layer, using the Softmax function, that will give us the probability distribution — where all probabilities sum to one — for each token. The final layer also transforms our LSTM output from a dimensionality of whatever we gave it (in our case, 10) to the number of unique words within the hidden layer’s vocabulary (i.e., the number of unique target tokens, which is definitely more than 10!).\n","\n","Keras’s implementation could work with several layer types, but Dense is the least complex, so we’ll go with that."]},{"cell_type":"code","metadata":{"id":"YRMIGAikyO7T"},"source":["from prep import num_encoder_tokens, num_decoder_tokens\n","\n","from tensorflow import keras\n","# Add Dense to the imported layers\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model\n","\n","# Encoder training setup\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder_lstm = LSTM(256, return_state=True)\n","encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n","encoder_states = [state_hidden, state_cell]\n","\n","# The decoder input and LSTM layers:\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n","\n","# Retrieve the LSTM outputs and states:\n","decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n","\n","# Build a final Dense layer:\n","decoder_dense = Dense(num_decoder_tokens, activation = \"softmax\")\n","\n","# Filter outputs through the Dense layer:\n","decoder_outputs = decoder_dense(decoder_outputs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BQrZ--3ykQ-"},"source":["##9.4 Build and Train seq2seq\n","\n","First, we define the seq2seq model using the Model() function we imported from Keras. To make it a seq2seq model, we feed it the encoder and decoder inputs, as well as the decoder output.\n","\n","Finally, our model is ready to train. First, we compile everything. Keras models demand two arguments to compile:\n","\n","- An optimizer (we’re using RMSprop, which is a fancy version of the widely-used gradient descent) to help minimize our error rate (how bad the model is at guessing the true next word given the previous words in a sentence).\n","\n","- A loss function (we’re using the logarithm-based cross-entropy function) to determine the error rate.\n","Because we care about accuracy, we’re adding that into the metrics to pay attention to while training.\n","\n","Next we need to fit the compiled model. To do this, we give the .fit() method the encoder and decoder input data (what we pass into the model), the decoder target data (what we expect the model to return given the data we passed in), and some numbers we can adjust as needed:\n","\n","- batch size (smaller batch sizes mean more time, and for some problems, smaller batch sizes will be better, while for other problems, larger batch sizes are better)\n","\n","- the number of epochs or cycles of training (more epochs mean a model that is more trained on the dataset, and that the process will take more time)\n","\n","- validation split (what percentage of the data should be set aside for validating — and determining when to stop training your model — rather than training)\n","Keras will take it from here to get you a (hopefully) nicely trained seq2seq model"]},{"cell_type":"code","metadata":{"id":"YgK9u_eAzHsy"},"source":["from prep import num_encoder_tokens, num_decoder_tokens, decoder_target_data, encoder_input_data, decoder_input_data, decoder_target_data\n","\n","from tensorflow import keras\n","# Add Dense to the imported layers\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model\n","\n","# Encoder training setup\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder_lstm = LSTM(256, return_state=True)\n","encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n","encoder_states = [state_hidden, state_cell]\n","\n","# Decoder training setup:\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n","decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Building the training model:\n","training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","print(\"Model summary:\\n\")\n","training_model.summary()\n","print(\"\\n\\n\")\n","\n","# Compile the model:\n","training_model.compile(optimizer = 'rmsprop',\n","             loss = 'categorical_crossentropy', \n","             metrics = ['accuracy'])\n","\n","# Choose the batch size\n","# and number of epochs:\n","batch_size = 50\n","epochs = 50\n","\n","print(\"Training the model:\\n\")\n","# Train the model:\n","training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HFGmc-FOzJvT"},"source":["##9.5 Setup for Testing\n","\n","Now our model is ready for testing! Yay! However, to generate some original output text, we need to redefine the seq2seq architecture in pieces. Wait, didn’t we just define and train a model?\n","\n","Well, yes. But the model we used for training our network only works when we already know the target sequence. This time, we have no idea what the Spanish should be for the English we pass in! So we need a model that will decode step-by-step instead of using teacher forcing. To do this, we need a seq2seq network in individual pieces.\n","\n","To start, we’ll build an encoder model with our encoder inputs and the placeholders for the encoder’s output states.\n","\n","Next up, we need placeholders for the decoder’s input states, which we can build as input layers and store together. Why? We don’t know what we want to decode yet or what hidden state we’re going to end up with, so we need to do everything step-by-step. We need to pass the encoder’s final hidden state to the decoder, sample a token, and get the updated hidden state back. Then we’ll be able to (manually) pass the updated hidden state back into the network.\n","\n","Using the decoder LSTM and decoder dense layer (with the activation function) that we trained earlier, we’ll create new decoder states and outputs.\n","\n","Finally, we can set up the decoder model. This is where we bring together:\n","\n","- the decoder inputs (the decoder input layer)\n","\n","- the decoder input states (the final states from the encoder)\n","\n","- the decoder outputs (the NumPy matrix we get from the final output layer of the decoder)\n","\n","- the decoder output states (the memory throughout the network from one word to the next)"]},{"cell_type":"code","metadata":{"id":"6SCl-CUSz2Hd"},"source":["from prep import num_encoder_tokens, num_decoder_tokens, decoder_target_data, encoder_input_data, decoder_input_data, decoder_target_data\n","\n","from tensorflow import keras\n","# Add Dense to the imported layers\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model\n","\n","# Encoder training setup\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder_lstm = LSTM(256, return_state=True)\n","encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n","encoder_states = [state_hidden, state_cell]\n","\n","# Decoder training setup:\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n","decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Building the training model:\n","training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","print(\"Model summary:\\n\")\n","training_model.summary()\n","print(\"\\n\\n\")\n","\n","# Compile the model:\n","training_model.compile(optimizer = 'rmsprop',\n","             loss = 'categorical_crossentropy', \n","             metrics = ['accuracy'])\n","\n","# Choose the batch size\n","# and number of epochs:\n","batch_size = 50\n","epochs = 50\n","\n","print(\"Training the model:\\n\")\n","# Train the model:\n","training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdUyV57Lz47e"},"source":["##9.6 The Test Function\n","\n","Finally, we can get to testing our model! To do this, we need to build a function that:\n","\n","- accepts a NumPy matrix representing the test English sentence input\n","\n","- uses the encoder and decoder we’ve created to generate Spanish output\n","\n","Inside the test function, we’ll run our new English sentence through the encoder model. The .predict() method takes in new input (as a NumPy matrix) and gives us output states that we can pass on to the decoder.\n","\n","Next, we’ll build an empty NumPy array for our Spanish translation, giving it three dimensions.\n","\n","Luckily, we already know the first value in our Spanish sentence — \"<Start>\"! So we can give \"<Start>\" a value of 1 at the first timestep.\n","\n","Before we get decoding, we’ll need a string where we can add our translation to, word by word.\n","\n","At long last, it’s translation time. Inside the test function, we’ll decode the sentence word by word using the output state that we retrieved from the encoder (which becomes our decoder’s initial hidden state). We’ll also update the decoder hidden state after each word so that we use previously decoded words to help decode new ones.\n","\n","To tackle one word at a time, we need a while loop that will run until one of two things happens (we don’t want the model generating words forever):\n","\n","- The current token is \"<END>\".\n","\n","- The decoded Spanish sentence length hits the maximum target sentence length.\n","\n","Inside the while loop, the decoder model can use the current target sequence (beginning with the \"<START>\" token) and the current state (initially passed to us from the encoder model) to get a bunch of possible next words and their corresponding probabilities.\n","\n","Next, we can use NumPy’s .argmax() method to determine the token (word) with the highest probability and add it to the decoded sentence.\n","\n","Our final step is to update a few values for the next word in the sequence.\n","\n","And now we can test it all out!"]},{"cell_type":"code","metadata":{"id":"EmO4Qgea0OH4"},"source":["from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens\n","\n","from prep import target_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, target_tokens\n","\n","from tensorflow import keras\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model, load_model\n","import numpy as np\n","\n","training_model = load_model('training_model.h5')\n","encoder_inputs = training_model.input[0]\n","encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","latent_dim = 256\n","decoder_state_input_hidden = Input(shape=(latent_dim,))\n","decoder_state_input_cell = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n","decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_hidden, state_cell]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n","\n","def decode_sequence(test_input):\n","  # Encode the input as state vectors:\n","  encoder_states_value = encoder_model.predict(test_input)\n","  # Set decoder states equal to encoder final states\n","  decoder_states_value = encoder_states_value\n","\n","  # Generate empty target sequence of length 1:\n","  target_seq = np.zeros((1, 1, num_decoder_tokens))\n","  \n","  # Populate the first token of target sequence with the start token:\n","  target_seq[0, 0, target_features_dict['<START>']] = 1.\n","  \n","  decoded_sentence = ''\n","\n","  return decoded_sentence\n","\n","for seq_index in range(10):\n","  test_input = encoder_input_data[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(test_input)\n","  print('-')\n","  print('Input sentence:', input_docs[seq_index])\n","  print('Decoded sentence:', decoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4Mp3iqn0vVE"},"source":["from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens\n","\n","from prep import target_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, target_tokens\n","\n","from tensorflow import keras\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model, load_model\n","import numpy as np\n","\n","training_model = load_model('training_model.h5')\n","encoder_inputs = training_model.input[0]\n","encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","latent_dim = 256\n","decoder_state_input_hidden = Input(shape=(latent_dim,))\n","decoder_state_input_cell = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n","decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_hidden, state_cell]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n","\n","def decode_sequence(test_input):\n","  encoder_states_value = encoder_model.predict(test_input)\n","  decoder_states_value = encoder_states_value\n","  target_seq = np.zeros((1, 1, num_decoder_tokens))\n","  target_seq[0, 0, target_features_dict['<START>']] = 1.\n","  decoded_sentence = ''\n","  \n","  stop_condition = False\n","  while not stop_condition:\n","    # Run the decoder model to get possible \n","    # output tokens (with probabilities) & states\n","    output_tokens, new_decoder_hidden_state, new_decoder_cell_state = decoder_model.predict(\n","      [target_seq] + decoder_states_value)\n","\n","    # Choose token with highest probability\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_token = reverse_target_features_dict[sampled_token_index]\n","    decoded_sentence += \" \" + sampled_token\n","\n","    # Exit condition: either hit max length\n","    # or find stop token.\n","    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n","      stop_condition = True\n","\n","    # Update the target sequence (of length 1).\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, sampled_token_index] = 1.\n","\n","    # Update states\n","    decoder_states_value = [new_decoder_hidden_state, new_decoder_cell_state]\n","\n","  return decoded_sentence\n","\n","for seq_index in range(11):\n","  test_input = encoder_input_data[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(test_input)\n","  print('-')\n","  print('Input sentence:', input_docs[seq_index])\n","  print('Decoded sentence:', decoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqnckgKafuub"},"source":["# 9. Rule-based chatbots"]},{"cell_type":"markdown","metadata":{"id":"9I0beFULf5i7"},"source":["## 9.0 Rule-based chatbots\n","\n","Basically, they need to have a closed domain and can understand a closed set of phrases. It is possible to use regex to increase user understanding."]},{"cell_type":"code","metadata":{"id":"mPnIFN95gSbi"},"source":["import re\n","import random\n","\n","class SupportBot:\n","  negative_responses = (\"nothing\", \"don't\", \"stop\", \"sorry\")\n","\n","  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\")\n","\n","  def __init__(self):\n","    self.matching_phrases = {'how_to_pay_bill': [r'.*how.*pay bills.*', r'.*how.*pay my bill.*'], r'pay_bill': [r'.*want.*pay.*my.*bill.*account.*number.*is (\\d+)', r'.*need.*pay my bill.*account.*number.*is (\\d+)']}\n","\n","  def welcome(self):\n","    name = input(\"Hi, I'm a customer support representative. Welcome to Codecademy Bank. Before we can help you, I need some information from you. What is your first name and last name? \")\n","    \n","    will_help = input(f\"Ok {name}, what can I help you with? \")\n","    \n","    if will_help in self.negative_responses:\n","      print(\"Ok, have a great day!\")\n","      return\n","    \n","    self.handle_conversation(will_help)\n","  \n","  def handle_conversation(self, reply):\n","    while not self.make_exit(reply):\n","      reply = self.match_reply(reply)\n","      \n","  def make_exit(self, reply):\n","    for exit_command in self.exit_commands:\n","      if exit_command in reply:\n","        print(\"Ok, have a great day!\")\n","        return True\n","      \n","    return False\n","  \n","  def match_reply(self, reply):\n","    for key, values in self.matching_phrases.items():\n","      for regex_pattern in values:\n","        found_match = re.match(regex_pattern, reply)\n","        if found_match and key == 'how_to_pay_bill':\n","          return self.how_to_pay_bill_intent()\n","        elif found_match and key == 'pay_bill':\n","          return self.pay_bill_intent(found_match.groups()[0])\n","        \n","    return input(\"I did not understand you. Can you please ask your question again?\")\n","  \n","  def how_to_pay_bill_intent(self):\n","    return input(\"You can pay your bill a couple of ways. 1) online at bill.codecademybank.com or 2) you can pay your bill right now with me. Can I help you with anything else?\")\n","  \n","  def pay_bill_intent(self, account_number=None):\n","    ACCOUNTNUMBER = account_number\n","    return input(f\"The account with number {ACCOUNTNUMBER} was paid off. What else can I help you with?\")\n","  \n","# Create a SupportBot instance\n","SupportConversation = SupportBot()\n","# Call the .welcome() method\n","SupportConversation.welcome()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rA6uRdMdgaqt"},"source":["#10. Retrieval-based chatbots"]},{"cell_type":"markdown","metadata":{"id":"a_B2lcAzglEl"},"source":["##10.0 Retrieval-based chatbots"]},{"cell_type":"markdown","metadata":{"id":"swuInv2Qgpw-"},"source":["# 11. Generative chatbots"]},{"cell_type":"markdown","metadata":{"id":"91m4sOWb1SKn"},"source":["##11.0 Choosing the right dataset\n","\n","One of the trickiest challenges in building a deep learning chatbot program is choosing a dataset to use for training."]},{"cell_type":"code","metadata":{"id":"H4Ur_K_01Zdi"},"source":["#data\n","\n","'''\n","L451317 +++$+++ u7151 +++$+++ m480 +++$+++ MOTHER +++$+++ You are sick, that's why he's here.\n","L451316 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ Mom, can't you tell him that I'm sick?\n","L451315 +++$+++ u7151 +++$+++ m480 +++$+++ MOTHER +++$+++ Your grandfather's here.\n","L451314 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ What?\n","L451313 +++$+++ u7151 +++$+++ m480 +++$+++ MOTHER +++$+++ Guess what.\n","L451312 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ A little bit.\n","L451311 +++$+++ u7151 +++$+++ m480 +++$+++ MOTHER +++$+++ You feeling any better?\n","L451417 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ What?\n","L451345 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ -hold it, hold it-\n","L451326 +++$+++ u7146 +++$+++ m480 +++$+++ GRANDFATHER +++$+++ That's right.\n","L451325 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ A book?\n","L451324 +++$+++ u7146 +++$+++ m480 +++$+++ GRANDFATHER +++$+++ Open it up.\n","L451323 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ What is it?\n","L451322 +++$+++ u7146 +++$+++ m480 +++$+++ GRANDFATHER +++$+++ I brought you a special present.\n","L451464 +++$+++ u7148 +++$+++ m480 +++$+++ INIGO +++$+++ I do not suppose you could speed things up?\n","L451463 +++$+++ u7149 +++$+++ m480 +++$+++ MAN IN BLACK +++$+++ Thank you.\n","L451462 +++$+++ u7148 +++$+++ m480 +++$+++ INIGO +++$+++ Sorry.\n","L452098 +++$+++ u7146 +++$+++ m480 +++$+++ GRANDFATHER +++$+++ As you wish...\n","L452097 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ Maybe you could come over and read it again to me tomorrow.\n","L452095 +++$+++ u7146 +++$+++ m480 +++$+++ GRANDFATHER +++$+++ Okay. Okay. Okay. All right. So long.\n","L452094 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ Okay.\n","L452093 +++$+++ u7146 +++$+++ m480 +++$+++ GRANDFATHER +++$+++ Now I think you ought to go to sleep.\n","L452088 +++$+++ u7153 +++$+++ m480 +++$+++ THE KID +++$+++ What? What?\n","L451438 +++$+++ u7155 +++$+++ m480 +++$+++ VIZZINI +++$+++ Inconceivable!\n","L451437 +++$+++ u7148 +++$+++ m480 +++$+++ INIGO +++$+++ He's climbing the rope. And he's gaining on us.\n","L451793 +++$+++ u7145 +++$+++ m480 +++$+++ FEZZIK +++$+++ Yeah?\n","L451792 +++$+++ u7148 +++$+++ m480 +++$+++ INIGO +++$+++ Perhaps not. I feel fine.\n","L451791 +++$+++ u7145 +++$+++ m480 +++$+++ FEZZIK +++$+++ You don't look so good.  You don't smell so good either.\n","L451790 +++$+++ u7145 +++$+++ m480 +++$+++ FEZZIK +++$+++ True!\n","L451789 +++$+++ u7148 +++$+++ m480 +++$+++ INIGO +++$+++ It's you.\n","L451788 +++$+++ u7145 +++$+++ m480 +++$+++ FEZZIK +++$+++ Hello.\n","L451915 +++$+++ u7150 +++$+++ m480 +++$+++ MIRACLE MAX +++$+++ A good hour. Yeah.\n","L451914 +++$+++ u7154 +++$+++ m480 +++$+++ VALERIE +++$+++ Yeah, an hour.\n","L451913 +++$+++ u7150 +++$+++ m480 +++$+++ MIRACLE MAX +++$+++ An hour.\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_exBeM811c75"},"source":["import more_itertools as mit\n","\n","data_path = \"pb.txt\"\n","\n","# Defining lines as a list of each line\n","with open(data_path, 'r', encoding='utf-8') as f:\n","  raw_lines = f.read().split('\\n')\n","\n","raw_lines.reverse()\n","lines = []\n","\n","for line in raw_lines:\n","    # split line into parts\n","    line_split = line.split(' +++$+++ ')\n","    # append tuple of character and line\n","    line_num = int(line_split[0][1:])\n","\n","    current_line = line_split[4].strip()\n","    # append tuple of line num, character and line\n","    lines.append((line_num, current_line))\n","# make sure the lines are in order\n","lines = sorted(lines, key=lambda x: x[0])\n","\n","# group lines by scene\n","by_scene = [list(group) for group in mit.consecutive_groups(lines, lambda x: x[0])]\n","\n","dialog_only = [[dialog_line[1] for dialog_line in dialog_group] \n","                for dialog_group in by_scene]\n","\n","dialog_combos_nested = [list(map(list, zip(dialog_group, dialog_group[1:]))) for dialog_group in dialog_only]\n","\n","dialog_combos = [combo for combos in dialog_combos_nested for combo in combos]\n","\n","# print dialog combos:\n","print(dialog_combos)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWyX-UOQ1gBm"},"source":["##11.1 Setting up the bot\n","\n","Just as we built a chatbot class to handle the methods for our rule-based and retrieval-based chatbots, we’ll build a chatbot class for our generative chatbot.\n","\n","Inside, we’ll add a greeting method and a set of exit commands, just like we did for our closed-domain chatbots.\n","\n","However, in this case, we’ll also import the seq2seq model we’ve built and trained on chat data for you, as well as other information we’ll need to generate a response.\n","\n","As it happens, many cutting-edge chatbots blend a combination of rule-based, retrieval-based, and generative approaches in order to easily handle some intents using predefined responses and offload other inputs to a natural language generation system."]},{"cell_type":"code","metadata":{"id":"PkUt-K1k1rqR"},"source":["class ChatBot:\n","  \n","  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n","\n","  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n","  \n","  def start_chat(self):\n","    user_response = input(\"Hi, I'm a chatbot trained on dialog from The Princess Bride. Would you like to chat with me?\\n\")\n","    \n","    if user_response in self.negative_responses:\n","      print(\"Ok, have a great day!\")\n","      return\n","    \n","    self.chat(user_response)\n","  \n","  def chat(self, reply):\n","    while not self.make_exit(reply):\n","      # change this line below:\n","      reply = input(self.generate_response(reply))\n","    \n","  # define .generate_response():\n","  def generate_response(self, user_input):\n","    return \"Cool!\\n\"\n","  \n","  def make_exit(self, reply):\n","    for exit_command in self.exit_commands:\n","      if exit_command in reply:\n","        print(\"Ok, have a great day!\")\n","        return True\n","      \n","    return False\n","  \n","# instantiate your ChatBot below:\n","chatty_mcchatface = ChatBot()\n","print(chatty_mcchatface.generate_response(\"Hey.\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUsexPUx1tWV"},"source":["##11.2 Generating Chatbot Responses\n","\n","As you may have noticed, a fundamental change from one chatbot architecture to the next is how the method that handles conversation works. In rule-based and retrieval-based systems, this method checks for various user intents that will trigger corresponding responses. In the case of generative chatbots, the seq2seq test function we built for the machine translation will do most of the heavy lifting for us!\n","\n","For our chatbot we’ve renamed decode_sequence() to .generate_response(). As a reminder, this is where response generation and selection take place:\n","\n","1. The encoder model encodes the user input\n","\n","2. The encoder model generates an embedding (the last hidden state values)\n","\n","3. The embedding is passed from the encoder to the decoder\n","\n","4. The decoder generates an output matrix of possible words and their probabilities\n","\n","5. We use NumPy to help us choose the most probable word (according to our model)\n","\n","6. Our chosen word gets translated back from a NumPy matrix into human language and added to the output sentence"]},{"cell_type":"code","metadata":{"id":"QaRdqB8b1-di"},"source":["import numpy as np\n","from seq2seq import encoder_model, decoder_model, num_decoder_tokens, target_features_dict, reverse_target_features_dict, max_decoder_seq_length\n","\n","class ChatBot:\n","  \n","  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n","\n","  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n","  \n","  def start_chat(self):\n","    user_response = input(\"Hi, I'm a chatbot trained on dialog from The Princess Bride. Would you like to chat with me?\\n\")\n","    \n","    if user_response in self.negative_responses:\n","      print(\"Ok, have a great day!\")\n","      return\n","    \n","    self.chat(user_response)\n","  \n","  def chat(self, reply):\n","    while not self.make_exit(reply):\n","      reply = input(self.generate_response(reply))\n","    \n","  # update .generate_response():\n","  def generate_response(self, user_input):\n","    states_value = encoder_model.predict(user_input)\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, target_features_dict['<START>']] = 1.\n","    \n","    chatbot_response = ''\n","\n","    stop_condition = False\n","    while not stop_condition:\n","      output_tokens, hidden_state, cell_state = decoder_model.predict(\n","        [target_seq] + states_value)\n","\n","      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","      sampled_token = reverse_target_features_dict[sampled_token_index]\n","      chatbot_response += \" \" + sampled_token\n","      \n","      if (sampled_token == '<END>' or len(chatbot_response) > max_decoder_seq_length):\n","        stop_condition = True\n","        \n","      target_seq = np.zeros((1, 1, num_decoder_tokens))\n","      target_seq[0, 0, sampled_token_index] = 1.\n","      \n","      states_value = [hidden_state, cell_state]\n","      \n","    return chatbot_response\n","  \n","  def make_exit(self, reply):\n","    for exit_command in self.exit_commands:\n","      if exit_command in reply:\n","        print(\"Ok, have a great day!\")\n","        return True\n","      \n","    return False\n","  \n","chatty_mcchatface = ChatBot()\n","# call .generate_response():\n","chatty_mcchatface.generate_response(\"I'd love to chat.\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYLmVgHJ2DkS"},"source":["##11.3 Handling user input\n","\n","Hmm… why can’t our chatbot chat? Right now our .generate_response() method only works with preprocessed data that’s been converted into a NumPy matrix of one-hot vectors. That won’t do for our chatbot; we don’t just want to use test data for our output. We want the .generate_response() method to accept new user input.\n","\n","Luckily, we can address this by building a method that translates user input into a NumPy matrix. Then we can call that method inside .generate_response() on our user input.\n","\n","We said it before, and we’ll say it again: we’re adding deep learning in now, so running the code may take a bit more time again."]},{"cell_type":"code","metadata":{"id":"a7wkV9Iv2KJp"},"source":["import numpy as np\n","import re\n","from seq2seq import encoder_model, decoder_model, num_decoder_tokens, num_encoder_tokens, input_features_dict, target_features_dict, reverse_target_features_dict, max_decoder_seq_length, max_encoder_seq_length\n","\n","class ChatBot:\n","  \n","  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n","\n","  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n","  \n","  def start_chat(self):\n","    user_response = input(\"Hi, I'm a chatbot trained on dialog from The Princess Bride. Would you like to chat with me?\\n\")\n","    \n","    if user_response in self.negative_responses:\n","      print(\"Ok, have a great day!\")\n","      return\n","    \n","    self.chat(user_response)\n","  \n","  def chat(self, reply):\n","    while not self.make_exit(reply):\n","      reply = input(self.generate_response(reply))\n","    \n","  # define .string_to_matrix() below:\n","  def string_to_matrix(self, user_input):\n","    tokens = re.findall(r'[\\w]+|[^\\s\\w]', user_input)\n","    user_input_matrix = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n","    for timestep, token in enumerate(tokens):\n","      user_input_matrix[0, timestep, input_features_dict[token]] = 1\n","    return user_input_matrix\n","  \n","  def generate_response(self, user_input):\n","    # change user_input into a NumPy matrix:\n","    user_input = self.string_to_matrix(user_input)\n","    # update argument for .predict():\n","    states_value = encoder_model.predict(user_input)\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, target_features_dict['<START>']] = 1.\n","    \n","    chatbot_response = ''\n","\n","    stop_condition = False\n","    while not stop_condition:\n","      output_tokens, hidden_state, cell_state = decoder_model.predict(\n","        [target_seq] + states_value)\n","\n","      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","      sampled_token = reverse_target_features_dict[sampled_token_index]\n","      chatbot_response += \" \" + sampled_token\n","      \n","      if (sampled_token == '<END>' or len(chatbot_response) > max_decoder_seq_length):\n","        stop_condition = True\n","        \n","      target_seq = np.zeros((1, 1, num_decoder_tokens))\n","      target_seq[0, 0, sampled_token_index] = 1.\n","      \n","      states_value = [hidden_state, cell_state]\n","      \n","    return chatbot_response\n","  \n","  def make_exit(self, reply):\n","    for exit_command in self.exit_commands:\n","      if exit_command in reply:\n","        print(\"Ok, have a great day!\")\n","        return True\n","      \n","    return False\n","  \n","chatty_mcchatface = ChatBot()\n","# call .generate_response():\n","print(chatty_mcchatface.generate_response(\"I love the princess\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrYSTbwi2R4D"},"source":["##11.4 Handling unkown words\n","\n","Nice work! Our chatbot now knows how to accept user input. But there is a pretty large caveat here: our chatbot only knows the vocabulary from our training data. What if a user uses a word that the chatbot has never seen before?\n","\n","With our current code, we’ll get a KeyError:\n","\n","This is because in .string_to_matrix() we are looking for token in input_features_dict.\n","\n","Currently, if the token doesn’t exist in the input_features_dict dictionary (which keeps track of all words in the training data), our program has no way of handling it.\n","\n","Here are a few popular approaches to tackle unknown words:\n","\n","- Tell the chatbot to ignore them, which is the simplest fix for smaller datasets, but can never generate those words as output. (Can you imagine scenarios when this could be a problem?)\n","\n","- Pause the chat process and have the chatbot ask what the entire utterance means. This requires the user to rephrase the entire utterance. This causes issues when working with a fairly limited dataset, since we may end up with the chatbot repeatedly asking the user to rephrase each input statement.\n","\n","- Add in a step for the chabot to register any unknown word as a '<UNK>' token. This is generally more complicated than the other two solutions. It would require that the training data is built out with '<UNK>' tokens and requires several extra manual steps."]},{"cell_type":"code","metadata":{"id":"7sxYCF_G2tj5"},"source":["import numpy as np\n","import re\n","from seq2seq import encoder_model, decoder_model, num_decoder_tokens, num_encoder_tokens, input_features_dict, target_features_dict, reverse_target_features_dict, max_decoder_seq_length, max_encoder_seq_length\n","\n","class ChatBot:\n","  \n","  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n","\n","  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n","  \n","  def start_chat(self):\n","    user_response = input(\"Hi, I'm a chatbot trained on dialog from The Princess Bride. Would you like to chat with me?\\n\")\n","    \n","    if user_response in self.negative_responses:\n","      print(\"Ok, have a great day!\")\n","      return\n","    \n","    self.chat(user_response)\n","  \n","  def chat(self, reply):\n","    while not self.make_exit(reply):\n","      reply = input(self.generate_response(reply))\n","    \n","  # define .string_to_matrix() below:\n","  def string_to_matrix(self, user_input):\n","    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n","    user_input_matrix = np.zeros(\n","      (1, max_encoder_seq_length, num_encoder_tokens),\n","      dtype='float32')\n","    for timestep, token in enumerate(tokens):\n","      # add an if clause to handle user input:\n","      if token in input_features_dict:\n","        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n","    return user_input_matrix\n","  \n","  def generate_response(self, user_input):\n","    input_matrix = self.string_to_matrix(user_input)\n","    states_value = encoder_model.predict(input_matrix)\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, target_features_dict['<START>']] = 1.\n","    \n","    chatbot_response = ''\n","\n","    stop_condition = False\n","    while not stop_condition:\n","      output_tokens, hidden_state, cell_state = decoder_model.predict(\n","        [target_seq] + states_value)\n","\n","      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","      sampled_token = reverse_target_features_dict[sampled_token_index]\n","      chatbot_response += \" \" + sampled_token\n","      \n","      if (sampled_token == '<END>' or len(chatbot_response) > max_decoder_seq_length):\n","        stop_condition = True\n","        \n","      target_seq = np.zeros((1, 1, num_decoder_tokens))\n","      target_seq[0, 0, sampled_token_index] = 1.\n","      \n","      states_value = [hidden_state, cell_state]\n","      \n","    return chatbot_response\n","  \n","  def make_exit(self, reply):\n","    for exit_command in self.exit_commands:\n","      if exit_command in reply:\n","        print(\"Ok, have a great day!\")\n","        return True\n","      \n","    return False\n","  \n","chatty_mcchatface = ChatBot()\n","# call .generate_response():\n","\n","print(chatty_mcchatface.generate_response(\"Now can I say love?\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m1t84L62gubn"},"source":["# Appendix: Web Scrapping with BeautifulSoup"]},{"cell_type":"markdown","metadata":{"id":"QB7WcyF-3qJr"},"source":["##A.1 Requests\n","\n","n order to get the HTML of the website, we need to make a request to get the content of the webpage. To learn more about requests in a general sense, you can check out this article.\n","\n","Python has a requests library that makes getting content really easy. All we have to do is import the library, and then feed in the URL we want to GET."]},{"cell_type":"code","metadata":{"id":"TFzlxDkM31Sa"},"source":["import requests\n","\n","webpage_response = requests.get(\"https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html\")\n","\n","webpage = webpage_response.content\n","\n","print(webpage)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jjk_NCIL32LL"},"source":["##A.2 The BeautifulSoup Object\n","\n","When we printed out all of that HTML from our request, it seemed pretty long and messy. How could we pull out the relevant information from that long string?\n","\n","BeautifulSoup is a Python library that makes it easy for us to traverse an HTML page and pull out the parts we’re interested in.\n","\n","\"html.parser\" is one option for parsers we could use. There are other options, like \"lxml\" and \"html5lib\" that have different advantages and disadvantages, but for our purposes we will be using \"html.parser\" throughout.\n","\n","With the requests skills we just learned, we can use a website hosted online as that HTML."]},{"cell_type":"code","metadata":{"id":"7tfqr9BG4IVB"},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html', 'html.parser')\n","\n","webpage = webpage_response.content\n","\n","soup = BeautifulSoup(webpage)\n","\n","print(soup)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oos3D9mH4K0w"},"source":["##A.3 Object types\n","\n","BeautifulSoup breaks the HTML page into several types of objects.\n","\n","A Tag corresponds to an HTML Tag in the original document. Accessing a tag from the BeautifulSoup object in this way will get the first tag of that type on the page. You can get the name of the tag using .name and a dictionary representing the attributes of the tag using .attrs.\n","\n","NavigableStrings are the pieces of text that are in the HTML tags on the page. You can get the string inside of the tag by calling .string.\n"]},{"cell_type":"code","metadata":{"id":"XmReusaA4ftF"},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html')\n","\n","webpage = webpage_response.content\n","soup = BeautifulSoup(webpage, \"html.parser\")\n","\n","print(soup.p.string)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6fU1R4l4hYF"},"source":["##A.4 Navigating by tags\n","\n","To navigate through a tree, we can call the tag names themselves.If we made a soup object out of this HTML page, we have seen that we can get the first h1 element by calling:\n","\n","print(soup.h1)\n","\n","We can get the children of a tag by accessing the .children attribute. We can also navigate up the tree of a tag by accessing the .parents attribute."]},{"cell_type":"code","metadata":{"id":"DurbFSwP6oy_"},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html')\n","\n","webpage = webpage_response.content\n","soup = BeautifulSoup(webpage, \"html.parser\")\n","\n","for child in soup.div.children:\n","  print(child)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dHc5Sae6xqj"},"source":["##A.5 Find all\n","\n","If we want to find all of the occurrences of a tag, instead of just the first one, we can use .find_all().\n","\n","This function can take in just the name of a tag and returns a list of all occurrences of that tag.\n","\n",".find_all() is far more flexible than just accessing elements directly through the soup object. With .find_all(), we can use regexes, attributes, or even functions to select HTML elements more intelligently.\n","\n","What if we want every <ol> and every <ul> that the page contains? We can select both of these types of elements with a regex in our .find_all().\n","\n","What if we want all of the h1 - h9 tags that the page contains? Regex to the rescue again!\n","\n","We can also just specify all of the elements we want to find by supplying the function with a list of the tag names we are looking for.\n","\n","We can also try to match the elements with relevant attributes. We can pass a dictionary to the attrs parameter of find_all with the desired attributes of the elements we’re looking for.\n","\n","If our selection starts to get really complicated, we can separate out all of the logic that we’re using to choose a tag into its own function. Then, we can pass that function into .find_all()!"]},{"cell_type":"code","metadata":{"id":"Bu2uQcgc7GiI"},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html')\n","\n","webpage = webpage_response.content\n","soup = BeautifulSoup(webpage, \"html.parser\")\n","\n","turtle_links = soup.find_all(\"a\")\n","\n","print(turtle_links)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1e6dVOb87HYl"},"source":["##A.6 Selector for SCSS Selectors\n","\n","Another way to capture your desired elements with the soup object is to use CSS selectors. The .select() method will take in all of the CSS selectors you normally use in a .css file!\n","\n","If we wanted to select all of the elements that have the class 'recipeLink', we could use the command:\n","\n","soup.select(\".recipeLink\")\n","\n","If we wanted to select the element that has the id 'selected', we could use the command:\n","\n","soup.select(\"#selected\")\n","\n","Let’s say we wanted to loop through all of the links to these funfetti recipes that we found from our search.\n","\n","for link in soup.select(\".recipeLink > a\"):\n","  webpage = requests.get(link)\n","  new_soup = BeautifulSoup(webpage)\n","\n","This loop will go through each link in each .recipeLink div and create a soup object out of the webpage it links to. So, it would first make soup out of <a href=\"spaghetti.html\">Funfetti Spaghetti</a>, then <a href=\"lasagna.html\">Lasagna de Funfetti</a>, and so on."]},{"cell_type":"code","metadata":{"id":"8ua7bY4X9IUp"},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","prefix = \"https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/\"\n","webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html')\n","\n","webpage = webpage_response.content\n","soup = BeautifulSoup(webpage, \"html.parser\")\n","\n","turtle_links = soup.find_all(\"a\")\n","links = []\n","#go through all of the a tags and get the links associated with them:\n","for a in turtle_links:\n","    links.append(prefix+a[\"href\"])\n","    \n","#Define turtle_data:\n","turtle_data = {}\n","\n","#follow each link:\n","for link in links:\n","  webpage = requests.get(link)\n","  turtle = BeautifulSoup(webpage.content, \"html.parser\")\n","  #Add your code here:\n","  turtle_name = turtle.select(\".name\")[0]\n","  turtle_data[turtle_name] = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DbSGCHND9NGp"},"source":["##A.7 Reading Text\n","\n","When we use BeautifulSoup to select HTML elements, we often want to grab the text inside of the element, so that we can analyze it. We can use .get_text() to retrieve the text inside of whatever tag we want to call it on.\n","\n","Notice that this combined the text inside of the outer h1 tag with the text contained in the span tag inside of it! Using get_text(), it looks like both of these strings are part of just one longer string. If we wanted to separate out the texts from different tags, we could specify a separator character. This command would use a . character to separate.\n","\n"]},{"cell_type":"code","metadata":{"id":"cPOBC2hd9VhA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616719510554,"user_tz":180,"elapsed":948,"user":{"displayName":"Lais Carraro Leme Cavalheiro","photoUrl":"","userId":"15498185604646105525"}},"outputId":"be57a0d4-026d-4a7e-af1a-7688a6c10046"},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","prefix = \"https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/\"\n","webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html')\n","\n","webpage = webpage_response.content\n","soup = BeautifulSoup(webpage, \"html.parser\")\n","\n","turtle_links = soup.find_all(\"a\")\n","links = []\n","#go through all of the a tags and get the links associated with them\"\n","for a in turtle_links:\n","    links.append(prefix+a[\"href\"])\n","    \n","#Define turtle_data:\n","turtle_data = {}\n","\n","#follow each link:\n","for link in links:\n","  webpage = requests.get(link)\n","  turtle = BeautifulSoup(webpage.content, \"html.parser\")\n","  turtle_name = turtle.select(\".name\")[0].get_text()\n","  \n","  stats = turtle.find(\"ul\")\n","  stats_text = stats.get_text(\"|\")\n","  turtle_data[turtle_name] = stats_text.split(\"|\")\n","\n","print(turtle_data)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Asia's largest titanium dioxide manufacture is trapped in an environmental vortex: Lomon Billions has been accused of sewage for many years\n"],"name":"stdout"}]}]}